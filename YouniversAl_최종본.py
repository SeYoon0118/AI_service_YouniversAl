'''
#í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤, í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í–‰
# YouTube ëŒ“ê¸€ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install youtube-comment-downloader

# ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (langid: í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)
!pip install langid

# Streamlit ì„¤ì¹˜ (Streamlitì€ ê°„ë‹¨í•œ ì›¹ ì•±ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)
!pip install streamlit

# ngrokì„ í†µí•´ ë¡œì»¬ ì„œë²„ë¥¼ ì™¸ë¶€ì—ì„œë„ ì ‘ì† ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (streamlit ì›¹ì•±ì„ ë°°í¬í•  ë•Œ ì‚¬ìš© ê°€ëŠ¥)
!pip install pyngrok

# ì–¸ì–´ ì½”ë“œ ê´€ë ¨ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (langcodes: ISO 639 ì–¸ì–´ ì½”ë“œ ê´€ë¦¬ ë° ë³€í™˜)
!pip install langcodes

!pip install transformer
'''
#ë””ìì¸ ì½”ë“œ
design_code = '''
import streamlit as st

def apply_css():
    st.markdown("""
    <style>
    body {
      margin: 0;
      padding: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background-color: #ffffff;
      text-align: center;
      color: #333;
    }
    .center-box {
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      margin-top: 50px;
    }
    .center-box img {
      width: 40px;
      margin-bottom: 20px;
    }
    .center-box .title {
      font-size: 48px;
      color: #e60000;
      font-weight: bold;
      margin: 10px 0;
    }
    .center-box .subtitle {
      font-size: 16px;
      color: #888;
      margin-bottom: 30px;
    }
    .input-style input {
      padding: 12px 20px;
      width: 300px;
      border-radius: 10px;
      border: 1px solid #ccc;
      font-size: 14px;
    }
    .button-style button {
      padding: 12px 20px;
      background-color: #e60000;
      color: white;
      border: none;
      border-radius: 10px;
      font-weight: bold;
      cursor: pointer;
      font-size: 14px;
    }
    .button-style button:hover {
      background-color: #cc0000;
    }
    </style>
    """, unsafe_allow_html=True)

def show_header():
    st.markdown("""
    <div class="center-box">
      <img src="https://upload.wikimedia.org/wikipedia/commons/b/b8/YouTube_Logo_2017.svg" alt="YouTube logo" />
      <div class="title">YouniversAI</div>
      <div class="subtitle">Your smart assistant for multilingual YouTube comment insights.</div>
    </div>
    """, unsafe_allow_html=True)
'''
with open("design.py", "w", encoding="utf-8") as f:
    f.write(design_code)

#ëŒ“ê¸€ìˆ˜ì§‘
comments_collection_code = '''
import os
import json
import pandas as pd

def get_comments(url):
    json_file = 'YoutubeComments.json'
    os.system(f'youtube-comment-downloader --url "{url}" --output {json_file}')

    with open(json_file, 'r', encoding='utf-8') as f:
        content = f.read()
        try:
            json_data = json.loads(content)
        except json.JSONDecodeError:
            json_data = []
            for line in content.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    json_data.append(json.loads(line))
                except json.JSONDecodeError:
                    continue

    df = pd.DataFrame(json_data)
    os.remove(json_file)
    return df
'''

# íŒŒì¼ë¡œ ì €ì¥
with open("logic.py", "w", encoding="utf-8") as f:
    f.write(comments_collection_code)

#ëŒ“ê¸€ ë¶„ë¥˜ë¥˜
classifier_code = '''
import langid
import langcodes

def classify_language(df):
    df['lang'] = df['text'].apply(lambda x: langid.classify(x)[0])
    df['ì–¸ì–´'] = df['lang'].apply(
        lambda code: langcodes.Language.get(code).display_name() if code else code)
    return df

def classify_and_store(df, session_state):
    df = classify_language(df)
    session_state.df = df
    return df
'''

# íŒŒì¼ë¡œ ì €ì¥
with open("classifier.py", "w", encoding="utf-8") as f:
    f.write(classifier_code)

#ìš”ì•½ ì „ìš© íŒŒì¼
summarizer_code='''
import re
import pandas as pd
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT

# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”
summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=-1  # CPU ëª¨ë“œ
)

# í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ì´ˆê¸°í™”
embed_model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
kw_model = KeyBERT(model=embed_model)

def summarize_by_language(df):
    result = {}
    # summarizer.py ìˆ˜ì •
    grouped = df.groupby('lang', dropna=True)


    for lang, group in grouped:
        comments = group['text'].dropna().astype(str).tolist()
        full_text = " ".join(comments)

        if len(full_text) < 50:
            summary = "(ëŒ“ê¸€ì´ ë„ˆë¬´ ì ì–´ ìš”ì•½ ë¶ˆê°€)"
            keywords = ""
        else:
            try:
                summary = summarizer(full_text[:1500], max_length=150, min_length=30, do_sample=False)[0]['summary_text']
            except:
                summary = "(ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ)"
            try:
                keywords = ", ".join([kw[0] for kw in kw_model.extract_keywords(full_text)])
            except:
                keywords = ""

        result[lang] = {
            "summary": summary,
            "keywords": keywords
        }

    return result
'''
# íŒŒì¼ë¡œ ì €ì¥
with open("summarizer.py", "w", encoding="utf-8") as f:
    f.write(summarizer_code)

#ê°ì • ë¶„ì„ ì „ìš© íŒŒì¼ì¼
sentiment_code='''
import langid
import pandas as pd
from transformers import pipeline

# ê°ì • ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” (CPU ëª¨ë“œ)
sentiment_model = "nlptown/bert-base-multilingual-uncased-sentiment"
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=sentiment_model,
    tokenizer=sentiment_model,
    truncation=True,
    device=-1
)

# ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ (ê°„ë‹¨ ë²„ì „)
def detect_langid(text):
    try:
        text = str(text).strip()
        if len(text) < 2:
            return 'unknown'
        return langid.classify(text)[0]
    except:
        return 'unknown'

# ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_sentiment(df):
    df['language'] = df['text'].apply(detect_langid)
    dfs_by_lang = {}

    for lang in df['language'].unique():
        df_lang = df[df['language'] == lang].copy()
        if df_lang.empty:
            continue

        comments = df_lang['text'].dropna().astype(str).tolist()
        if not comments:
            continue

        try:
            results = sentiment_pipeline(comments)
        except Exception:
            continue

        df_lang['sentiment'] = [r['label'] for r in results]
        df_lang['score'] = [r['score'] for r in results]
        df_lang['sentiment_score'] = df_lang['sentiment'].apply(lambda l: int(l.split()[0]))
        df_lang['sentiment_kor'] = df_lang['sentiment_score'].apply(
            lambda s: "ë¶€ì •" if s <= 2 else ("ì¤‘ë¦½" if s == 3 else "ê¸ì •")
        )

        dfs_by_lang[lang] = df_lang

    if not dfs_by_lang:
        return df.copy()

    return pd.concat(dfs_by_lang.values(), ignore_index=True)
'''
# íŒŒì¼ë¡œ ì €ì¥
with open("sentiment.py", "w", encoding="utf-8") as f:
    f.write(sentiment_code)




'''
#ìµœì¢… ì‹¤í–‰ íŒŒì¼(ê°ì •ë¶„ì„+ìš”ì•½ì•½)
!pip install keybert sentence-transformers transformers --quiet #í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í–‰
'''
streamlit_code='''
import streamlit as st
import langcodes
from design import apply_css, show_header
from logic import get_comments
from classifier import classify_and_store
from summarizer import summarize_by_language
from sentiment import analyze_sentiment  # âœ… ê°ì • ë¶„ì„ ì¶”ê°€

def main():
    apply_css()
    show_header()

    url = st.text_input("Paste YouTube video URL here")
    if st.button("Go"):
        st.video(url)  # âœ… ìœ íŠœë¸Œ ì˜ìƒ í‘œì‹œ
        st.write(f"\U0001F4FA You entered: {url}")
        with st.spinner("\U0001F50EëŒ“ê¸€ì„ ë¶„ë¥˜ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
            df = get_comments(url)
            if df.empty or 'text' not in df.columns:
                st.warning("No comments found or 'text' field missing.")
                return
            classify_and_store(df, st.session_state)

    if 'df' in st.session_state:
        df = st.session_state.df
        st.success("ëŒ“ê¸€ ìˆ˜ì§‘ ë° ì–¸ì–´ ë¶„ë¥˜ ì™„ë£Œ!")
        st.write("ì´ ëŒ“ê¸€ ìˆ˜:", len(df))

        languages = df['lang'].unique().tolist()
        language_options = [langcodes.Language.get(code).display_name() for code in languages]

        selected_langs = st.multiselect("ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”", options=language_options, default=language_options)
        selected_lang_codes = [code for code in languages if langcodes.Language.get(code).display_name() in selected_langs]
        filtered_df = df[df['lang'].isin(selected_lang_codes)]

        if not filtered_df.empty:
            st.write(filtered_df[['ì–¸ì–´', 'text']])

            # âœ… ê°ì • ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
            if st.button("ê°ì • ë¶„ì„í•˜ê¸°"):
                with st.spinner("\U0001F914 ê°ì •ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    df_with_sentiment = analyze_sentiment(filtered_df)
                    st.session_state.df = df_with_sentiment  # ì—…ë°ì´íŠ¸
                    st.success("ê°ì • ë¶„ì„ ì™„ë£Œ!")
                    st.dataframe(df_with_sentiment[['ì–¸ì–´', 'text', 'sentiment_kor']])

            # âœ… ìš”ì•½ ë²„íŠ¼ ì¶”ê°€
            if st.button("ìš”ì•½í•˜ê¸°"):
                with st.spinner("\u23F3 ì–¸ì–´ë³„ ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.session_state.summary = summarize_by_language(filtered_df)

                for lang_code in selected_lang_codes:
                    summary = st.session_state.summary.get(lang_code)
                    lang_label = langcodes.Language.get(lang_code).display_name()
                    st.markdown(f"### ğŸ’¬ {lang_label} ìš”ì•½")
                    if summary:
                        st.markdown(f"**ìš”ì•½:** {summary.get('summary', '(ìš”ì•½ ì—†ìŒ)')}")
                        st.markdown(f"**í‚¤ì›Œë“œ:** {summary.get('keywords', '(í‚¤ì›Œë“œ ì—†ìŒ)')}")
                    else:
                        st.info("ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        else:
            st.warning("ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
'''
# ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥
with open("streamlit_app.py", "w", encoding="utf-8") as f:
    f.write(streamlit_code)
