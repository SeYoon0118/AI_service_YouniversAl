import streamlit as st
import os
import json
import pandas as pd
import langid
import langcodes
import re
from transformers import pipeline
from langdetect import detect
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ ê°ì • ë¶„ì„ & ìš”ì•½ íŒŒì´í”„ë¼ì¸(CPU ëª¨ë“œ) â”€â”€
sentiment_model = "nlptown/bert-base-multilingual-uncased-sentiment"
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=sentiment_model,
    tokenizer=sentiment_model,
    truncation=True,
    device=-1             # CPU ëª¨ë“œ ê°•ì œ
)

summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=-1             # CPU ëª¨ë“œ ê°•ì œ
)

# â”€â”€ KeyBERTë„ CPU ì „ìš©ìœ¼ë¡œ ì´ˆê¸°í™” â”€â”€
embed_model = SentenceTransformer(
    "all-MiniLM-L6-v2",  # or your preferred SBERT model
    device="cpu"         # â† ê¼­ CPU ëª¨ë“œë¡œ ì§€ì •
)
kw_model = KeyBERT(model=embed_model)


# -------------------- ì „ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ --------------------

def is_korean_internet_slang(text):
    if pd.isna(text):
        return False
    text = str(text).strip()
    return (
        bool(re.fullmatch(r'[ã„±-ã…ã…-ã…£ã…œã… ã…‹ã…\s]+', text))
        or any(kw in text for kw in ['ã„¹ã…‡', 'ã…‡ã…ˆ', 'ã……ã…‚', 'ã…‹ã…‹ã…‹', 'ã„·ã„·'])
    )

def clean_text(text):
    if pd.isna(text):
        return ''
    return re.sub(r'[^\w\sã„±-í£]', '', str(text))

def detect_langid_custom(text):
    try:
        text = str(text).strip()
        if len(text) < 2:
            return 'unknown'
        if is_korean_internet_slang(text):
            return 'ko'
        cleaned = clean_text(text)
        lang, _ = langid.classify(cleaned)
        return lang
    except:
        return 'unknown'

def get_comments(url):
    json_file = 'YoutubeComments.json'
    os.system(f'youtube-comment-downloader --url "{url}" --output {json_file}')
    with open(json_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        data = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    os.remove(json_file)
    return pd.DataFrame(data)

def analyze_sentiment(df):
    # ì–¸ì–´ ê°ì§€ í›„ ë¹ˆ ë°ì´í„° ì²˜ë¦¬
    df['language'] = df['text'].apply(detect_langid_custom)
    dfs_by_lang = {}
    for lang in df['language'].unique():
        df_lang = df[df['language'] == lang].copy()
        if df_lang.empty:
            continue

        comments = df_lang['text'].dropna().astype(str).tolist()
        if not comments:
            continue

        try:
            results = sentiment_pipeline(comments)
        except Exception as e:
            # ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ê°€ ë‚˜ë©´ ì´ ì–¸ì–´ëŠ” ê±´ë„ˆë›°ê¸°
            continue

        df_lang['sentiment'] = [r['label'] for r in results]
        df_lang['score'] = [r['score'] for r in results]
        df_lang['sentiment_score'] = df_lang['sentiment'].apply(lambda l: int(l.split()[0]))
        df_lang['sentiment_kor'] = df_lang['sentiment_score'].apply(
            lambda s: "ë¶€ì •" if s <= 2 else ("ì¤‘ë¦½" if s == 3 else "ê¸ì •")
        )
        dfs_by_lang[lang] = df_lang

    if not dfs_by_lang:
        # ë§Œì•½ í•˜ë‚˜ë„ ë‚¨ì€ ì–¸ì–´ê°€ ì—†ë‹¤ë©´ ì›ë³¸ df(ì–¸ì–´Â·ê°ì • ì—†ëŠ” ìƒíƒœ) ë¦¬í„´
        return df.copy()
    return pd.concat(dfs_by_lang.values(), ignore_index=True)

def summarize_by_language(df):
    result = {}
    # ì–¸ì–´ ì½”ë“œë³„ë¡œ ê·¸ë£¹í•‘
    grouped = df.groupby('language', dropna=True)
    for lang, group in grouped:
        comments = group['text'].dropna().astype(str).tolist()
        full_text = " ".join(comments)

        if len(full_text) < 50:
            summary = "(ëŒ“ê¸€ì´ ë„ˆë¬´ ì ì–´ ìš”ì•½ ë¶ˆê°€)"
            keywords = ""
        else:
            try:
                # 1,500ìê¹Œì§€ë§Œ ì˜ë¼ì„œ ìš”ì•½
                summary = summarizer(full_text[:1500], max_length=150, min_length=30, do_sample=False)[0]['summary_text']
            except Exception as e:
                summary = "(ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ)"
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                keywords = ", ".join([kw[0] for kw in kw_model.extract_keywords(full_text)])
            except:
                keywords = ""
        result[lang] = {"summary": summary, "keywords": keywords}
    return result

# ------------------------ Streamlit ì›¹ UI ------------------------

def main():
    st.title("YouniversAI")
    url = st.text_input("Paste YouTube video URL here")

    if st.button("Go"):
        with st.spinner("ğŸ” ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            df = get_comments(url)
            if df.empty or 'text' not in df.columns:
                st.warning("No comments found.")
                return

            # â”€â”€ ê°ì • ë¶„ì„ ë° ì–¸ì–´ ì´ë¦„ ì¶”ê°€ â”€â”€
            df = analyze_sentiment(df)

            def get_lang_name(code):
                try:
                    return langcodes.Language.get(code).display_name("en")
                except:
                    return "Unknown"

            df['lang_name'] = df['language'].apply(get_lang_name)
            st.session_state.df = df

            # â”€â”€ ì–¸ì–´ë³„ ìš”ì•½ ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ìºì‹œ ì €ì¥ â”€â”€
            st.session_state.summary = summarize_by_language(df)

    if 'df' in st.session_state:
        df = st.session_state.df
        st.success("âœ… ëŒ“ê¸€ ë¶„ì„ ì™„ë£Œ!")
        st.write(f"ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}")

        # ì–¸ì–´ ì„ íƒ UI: ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ì´ë¦„
        lang_options = df['lang_name'].dropna().unique().tolist()
        selected_langs = st.multiselect("ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”", options=lang_options, default=lang_options)

        # ì–¸ì–´ ì´ë¦„ â†’ ì–¸ì–´ ì½”ë“œ ë§¤í•‘
        def safe_display_name(code):
            try:
                return langcodes.Language.get(code).display_name("en")
            except:
                return None

        lang_name_to_code = {}
        for code in df['language'].dropna().unique():
            name = safe_display_name(code)
            if name:
                lang_name_to_code[name] = code

        selected_codes = [lang_name_to_code[name] for name in selected_langs if name in lang_name_to_code]
        filtered_df = df[df['language'].isin(selected_codes)]

        if not filtered_df.empty:
            st.write("ğŸ“‹ í•„í„°ë§ëœ ëŒ“ê¸€")
            st.dataframe(filtered_df[['lang_name', 'text', 'sentiment_kor']])

            # ì–¸ì–´ë³„ ìš”ì•½Â·í‚¤ì›Œë“œ ì¶œë ¥
            for lang_code in selected_codes:
                summary = st.session_state.summary.get(lang_code)
                lang_label = safe_display_name(lang_code) or "Unknown"
                st.markdown(f"### ğŸ’¬ {lang_label} ìš”ì•½")

                if summary:
                    st.markdown(f"**ìš”ì•½:** {summary.get('summary', '(ìš”ì•½ ì—†ìŒ)')}")
                    st.markdown(f"**í‚¤ì›Œë“œ:** {summary.get('keywords', '(í‚¤ì›Œë“œ ì—†ìŒ)')}")
                else:
                    st.info("ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
